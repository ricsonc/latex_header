\title{Paper Notes}
\author{Ricson}
\date{\today}
\documentclass[12pt]{article}

\input{headers}

\begin{document}
\maketitle

High level summaries of a papers, with a focus on the high level methodology and a de-emphasis on results. For conciseness some of the paper titles have been highly abbreviated.

\tableofcontents

\section{Reinforcement Learning}

\subsection{Rope Manipulation \cite{RopeManipulation}}

The main objective of rope manipulation is for a robot to move a rope according to a human demonstration, so that the robot can learn to perform complex tasks and manipulations by observing a human. The reason why a rope is used, is because a rope is a non-rigid body, which makes the task more challenging.

This problem cannot be neatly solved by traditional policy gradients, because policy gradients requires a huge amount of training data, and we may only have one, or a small number of demonstrations. This problem can also not be solved by optimal control techniques, because optimal control usually requires some form of state knowledge. It is very difficult to parameterize the state of the rope in a principled way.

The key idea of this paper is that the robot should learn how to move the rope on a small scale, so it is capable of performing simple actions to get from one state to another similar state. Then, the human demonstration, which is perhaps a video of a human manipulating the rope, is viewed as a set of instructions about what moves to make. Each consecutive key frame pair in the video can be given to the robot, so that it manipulates the rope correctly between those two frames.

This requires something known as an inverse dynamics model: given the previous world state and the next world state, what is the action which will cause the next world state to happen? We can train such an inverse dynamics model as follows: first, starting from frame 1, make a random move u. Then we arrive at frame 2. Feed frame 1 and frame 2 into our model to predict u, and take the loss from there.

The inverse dynamics model is called ``pixel-level'' because the state of the world is fed in as the camera input, without any explicit state encoding mechanism.

The authors used a notable trick in order to obtain training data biased in ways which might be useful. First, a very small dataset of interesting rope positions was generated by hand. Then, the inverse dynamics model was run in order to attempt to move the rope from one interesting position to another interesting position. The resulting position from applying the inverse dynamics model was added to the dataset, and the process was repeated. 

\subsection{Learning To Poke \cite{LearningToPoke}}

The main objective of this paper was to train a forward dynamics model. In optimal control, the dynamics model is assumed to be linear, and the parameters are learned by linear regression. Some other papers have trained autoencoders in order to map image-space into a new feature space where the dynamics are indeed linear. However, it may be interesting to learn more unconstrained, nonlinear dynamics.

However, for the same reason that linear dynamics is a poor assumption in image space, it's also very difficult to train a general dynamics model in image space. Intuitively, it is much harder to know exactly what a cup will look like when it is upside down -- at the pixel level -- than it is to know semantically what it means for the cup to be upside down.

The paper describes an encoder network which encodes image space into a new feature space. Then, an dynamics model is trained. At first, this seems like a bad idea, because the encoder network could map every image into the same feature vector, and hence achieve 0 loss.

Therefore, it is necessary to also simultaneously train an inverse dynamics model, to infer the action taken based off of the feature vectors. This also restrains the feature vectors to be reasonable and non-constant, otherwise the inverse dynamics model will fail.

\subsection{Emergence of Grounded Compositional Language \cite{EGCLMAP}}

In a nutshell, this paper is about getting multiple agents to coordinate with each other in order to accomplish their respective goals by using language. Since the language is learned solely to accomplish these tasks, we say it is both emergent and grounded in the concepts necessary to accomplish these tasks. In additional the language is compositional, so putting words to gether can create complex meaning (as opposed to having one word for every concept in the language).

The setup is a POMDP environment with multiple agents. Each agent has their own goal, and the reward of the the agents is the sum of the rewards of each goal. In other terms, the agents are cooperative.

The game itself has $N$ agents and $M$ landmarks, each agent and landmark having position $p$, and each agent can look at position $v$. Furthermore, the description of each entity (agent or landmark) is represented by $x$, which contains $p$, $\dot p$ (which is change in position), $v$, and $d$ -- the color of each entity. Each timesteps, agents can (by exerting some force vector), and change their gaze to another position. The movement dynamics seem to be differentiable.

Agents can also talk to each other by emitting symbols from some fixed alphabet of size $K$. There is a negative reward for talking, to incentivise useful communication only.

There is also some state which is private to each agent, so that observations drawn from the state by a certain agent will not contain the private states of other agents. The goal of each agent and the memory of each agent is private.

The goal of each agent is for some other agent to either go to a location, look at a location, or do nothing. Assuming the goal is to go to a location, the specification of the goal contains the following: The id of the agent which must go to some place, the location it must go to, and the variable indicating the goal-type.

To summarize, the goal consists of four parts: the entity encoding $x$, the words that were just said $c$, the private memory $m$, and the private goals $g$. 

The authors attempt to solve this reinforcement learning with a fully differential training mechanism. As is usually the case with reinforcement learning problems, the struggle is in the differentiability of the environment.

First, with smoothing of physics, the physical position and velocity of each entity can be made differentiable. Second, the goals of each agent are constant (and hence differentiable). The memory bank of each agent can be made differentiable as long as the part of the agent which updates the memory bank is differentiable. Finally, the challenge comes down to the communication, which takes place using discrete symbols, which are not differentiable (you might imagine they are sampled from some softmax distribution). The authors solve this problem using the gumbel-softmax trick to reparameterize.

Now we will study the parameterization of the agents' policies. The architecture must output a physical movement, a communication, a gaze change, and a memory update. As input to the model is the observations which we described before.

The architecture is composed of multiple processing modules, each of which has it's own portion of the memory bank. Each module is a feed-forward fully connected network which takes in both some inputs, and also the memory in the bank. It outputs a new memory and some feature vector.

For simplicity, assume there is only one agent and no landmarks in the simulation. Then the network architecture will be a tree with a root node and two leaves. At one of the leaves, the inputs is the entity descriptions $x$, which are fed into a processing module. At the other leaf, the input is the communication $c$ which is fed into another processing module (weights are not shared!). Finally, the outputs of both processing modules, along with the goal of the agent, is fed into a third processing module at the root of the tree, which produces the new actions and communications.

When there is more than one entity from the entity description or from the communications, then the architecture becomes more complex. For example, the entity branch of the network instantiates $N+M$ processing modules -- one for each entity in the game. The weights of all $N+M$ processing modules are shared, but the memory is not shared. The outputs of all these processing modules is collapsed to a single feature vector using something like softmax. The same thing applies for the communication branch.

In order to help the agents, the authors included an auxilary branch on each agent which also predicts the goal vector of all the other agents. Agents are rewarded for correct predictions.

Now we will discuss how the authors managed to elicit compositional language from the agents. The key is to penalize unnecessarily large vocabularies, so that a language which is compositional and concepts are ``factorized'' results in better rewards.

Dirichlet process.

\todo{finish this}

Finally, on to the results.

\section{Theoretical}
\subsection{Embodied Cognition \cite{EmbodiedCognition}}

The authors of this paper list six things which babies do which may be important or essential for developing an intelligent agent.

The first key is multi-modality. In other words, having multiple senses and being able to use them all simultaneously can be extremely useful. According to my reading, the reason for this is because multiple modalities overconstrain each other, allowing many opportunities for self-supervision. 

The second key is incremental learning. (See curriculum learning techniques). This section of the paper was quite unclear. The author provided several examples of how babies gradually learn to integrate and refine their ability to use their senses over a period of several months/years.

The third key is being physical. Again, the authors were quite unclear about this, and provided an example where babies were able to associate both a name and an object by virtue that they were both presented on the right side, versus the left side.

The fourth key is exploration. This point seems relatively straightforward -- babies spend a lot of time playing with toys for seemingly no reason, however, this time is vital for development of cognition. This point has been explored quite a lot in the literature (see Learning To Poke).

The fifth key is socialization. According to the authors, this seems to be mostly useful because parents can teach infants how language and objects correspond to each other, accelerating the learning process. Essentially, this is the whole point of language grounding.

The sixth key is to learn a language. The author's don't actually discuss why a language is important, but rather the idea that a language should be orthogonal to a concept. For example, the word ``dog'' is not similar to the object dog, and the word ``dig'', although sounding similar to the word ``dog'', does not have anything to do with a dog. Although the authors didn't exxplain why, they claimed that the language being orthogonal to the concepts described by the language accelerated the learning of abstractions. 

\subsection{Machine Reasoning \cite{MachineReasoning}}

According to the author, machine learning techniques have not managed to match human cognition so far. Limitations of logical inference techniques (such as symbolic AI), are computational intractibility. A limitation of the more recently used probabilistic inference techniques is the difficulty of expressing causation using probabilities.

In the second section, the author describes a clever trick in order to train a face classification model when the amount of directly labeled data is very small, but we can make use of a large amount of labels from an auxilary task. The auxilary task is telling if the face in two different images is the same.

First, all the images in both tasks are passed through a common preprocessor module P, which compresses the image into a feature vector. The face discriminator which tells if the two faces are the same or different takes as input these feature vectors, so P is trained to come up with features very useful for discriminating faces. This makes the job of the face classifier C that we want to train on our original task very easy, since it can take as input the output of P, which has already done most of the work.

Notice that each module of this system (there are three modules: C, P, and D), are learning something, and the design process of the system is to decide how to arrange the modules in the right way. In the third section of the paper, the author makes the claim that reasoning is the algebraic manipulation of knowledge. For example, the arrangement of modules into a face recognition system is an example of reasoning. We want to be able to learn to arrange the modules in the right way instead of relying on humans to do it for us.

The rest of the paper provides more speculation and details about how such a model might look and what the useful modules might be. 

\section{Natural Language Processing}

\subsection{Probabilistic Language Model \cite{ProbabilisticLanguageModel}}

This paper contains the foundational backbone of many modern language models and in particular, the word2vec model. The key idea is to learn a distributed representation for words, since a one-hot representation of words is very expensive. 

The fundamental task is to model the joint probability distribution of a sequence of words:

\eq{
  p(x_1, \cdots x_T) &= \prod_t p(x_t | x_1, \cdots x_{t-1})
}

In a naive model, we might approximate this with a more computationally tractable form:

\eq{
  p(x_1, \cdots x_T) &= \prod_t p(x_t | x_{t-1}, x_{t-2}, \cdots x_{t-k})
}

Where $k$ is the number of previous words that matter. Now it remainds to model $p(x_t | x_{t-1}, x_{t-2}, \cdots x_{t-k})$. In the naive approach, the skipgram approach, we simply use counting / tabulation in order to compute this probability.

If we represent each word with a vector, we could more cleverly model the probability as:

\eq{
  p(x_t | x_{t-1}, x_{t-2}, \cdots x_{t-k}) &= g(Cx_{t-1}, \cdots, Cx_{t-k}) 
}

where $C$ is a matrix containing all the word-vectors, each $x$ is represented as a one-hot vector, and $g$ is parameterized as any neural network with a softmax across the last layer.

\subsection{word2vec \cite{word2vec}}

The authors of this paper noticed that most of the computational complexity from the previous paper came from the hidden layer in the model, which is typically on the order of $10^3$ units. If the output softmax is on the order of $10^4$ units and the input may be on the order of $10^4$ units, then the total cost may be something like $10^7$.

Note that there are other solutions to this complexity problem, namely hierarchical softmax to shrink the output layer size and recurrent models to shrink the input layer size.

This paper proposes two better solutions:

CBOW, or continuous bag of words, replaces the hidden layer with a simple averaging of the input vectors. This makes all permutations of the previous few words equivalent. In addition, a further adjustment is that CBOW takes into input several words before and also several words after the current word (this is called the context). Hence it no can be used to define a joint probability distribution on word sequences -- instead, the purpose is purely to create word embeddings. In equation form, CBOW aims to maximize the following:

\eq{
  &\max \prod_t p(w_t | c_t) \\
  &\ra \max \sum_t \log p(w_t | c_t) \\
  \intertext{where $c_t$ is defined as $w_{t-m}, \cdots, w_{t-1}, w_{t+1}, \cdots , w_{t+m}$}
  p(w_t|c_t) &= g\paren{\frac{1}{n} \sum_{j \in c_t} Cw_j}
}

where $g$ is a simple linear transform followed by a (hierarchical) softmax.

Skip-grams do the reverse of CBOW -- instead of using the context to predict the current word, skip-gram predicts the context from the current word:


\eq{
  &\max \prod_t p(c_t | w_t) \\
  &\ra \max \sum_t \log p(c_t | w_t) \\
  p(c_t|w_t) &= \prod_{j \in c_t} g(Cw_t) \\
}

where $g$ is again, a simple linear transform followed by a softmax. 

\subsection{Dynamic Memory Networks \cite{DynamicMemory}}

This paper seems to be an exercise in cramming as many distinct RNNs into a model as possible. The task is to answer a given text question, with a text answer, given some information (in text). There are four main modules: The input module, the question module, the episodic memory module, and the answer module.

The input module takes the input information as input, and produces as output a sequence of vectors (one for each sentence in the input). The question module takes as input the question and outputs a single vector. The episodic memory module takes as input both the output of the input module (a sequence of vectors), and the output of the question module (a single vector), and outputs a single vector. Finally, the answer module takes as input the output of the episodic memory module and outputs a sequence of words.

The input module operates the input sentences by concatenating all the sentences together, and then running an RNN across the entire input. At the end of every sentence, the hidden state is saved. Therefore, if there are 3 sentences, the input module produces 3 vectors from the hidden state of the RNN -- one at the end of every word. Note that both the input and the question model take as input pre-embedded word vectors. 

The question module is your typical LSTM which takes a sequence and spits out a vector. In the opposite fashion, the answer module is an LSTM which takes in a vector and spits out a sequence (one-to-many).

The episodic-memory module is the most complex component of the system, and involves two RNNs and an attention mechanism. In addition, the episodic-memory module produces a vector $m$ at the end of every episode. In a subsequent episode, the previous vector $m$ is used to control the attention of the system. The final memory vector $m$ is the output of this module. The first RNN in the episodic-memory module iterates across the input vectors, while the second RNN in the episodic-memory module iterates across the episodes. It is here that the attention mechanism takes place: instead of the hidden state of the first RNN being updated normally, it is updated by taking a weighted average of the tentative ``new hidden state'' and the last hidden state, where the weighting is determined by a gating network. The gating network depends on the previous memory vector, the question, and the current input vector that the first RNN is processing. The second RNN takes as input the last hidden state of the first RNN. 

\section{Miscellaneous}

\subsection{Pointer Networks \cite{PointerNetworks}}

A traditional sequence to sequence model tends to have a fixed output alphabet size. Pointer networks were designed to accomodate variable output alphabet sizes -- specifically, the output alphabet size is exactly the length of the input. For example, solving the traveling salesman problem or sorting a variable length list -- both problems where the answer is a permutation of the input list -- requires an output alphabet with size equal to the size of the input.

A typical sequence to sequence model is LSTM with attention. If the encoder hidden states are denoted $e_1, \cdots, e_n$ and the decoder hidden states $d_1, \cdots, d_n$, then typically, the attention augmented decoder hidden state is computed at time $j$ as follows:

\eq{
  u_i &= v^T \sigma(W_1 e_i + W_2 d_j)
  \intertext{(where $v$ and both $W$'s are learnable weights)}
  a_i &= \smx(u_i)
  \intertext{These are the weights on each encoder hidden state, which is the ``context''}
  d'_j &= \sum_i a_i e_i
}

The pointer-network modifies the attention mechanism to directly output the distribution of the $j$th symbol $o_j$:

\eq{
  u_i &= v^T \sigma(W_1 e_i + W_2 d_j) \\
  o_i &\sim \smx(u_i) 
}


The authors were able to test pointer-networks trained on tasks of one size $n$ and then test them on tasks with size $n' > n$, and demonstrated successful generalization.

\section{Semantic Parsing}

\subsection{Language to Logical Form \cite{Language2LogicalForm}}

The task in this paper is semantic parsing, which is the conversion of an english sentence into some formal, machine readable language (such as SQL). The obvious way to do this is your favorite sequence to sequence model. However, sequence to sequence models may not be the correct approach, because programming languages may be better viewed as trees rather than sequences (indeed, program representation as abstract syntax trees is quite common). The authors introduce a sequence2tree model which accomplishes this, and achieve modest improvement on the task. All of this is augmented with your standard attention mechanism. 

The sequence2tree model doesn't make any changes to the encoder, but it does make changes to the decoder. Consider the string ``(a (b c) (d e))'' This is perhaps better viewed as a tree where the root note has three subtrees. The first subtree consists of only the leaf node ``a'' where as the other two subtrees have two leaves branching from their root notes. In other words, the sequence2tree model seeks to generate the following output: ``(a [n] [n])'', where each special symbol ``[n]'' is a placeholder. The we should then make a second pass over the sentence to replace the first ``[n]'' with ``(b c)'' and the second with ``(d e)''.

Now we can go into the details of how this is done. Recall that the decoder LSTM only needs the final hidden state vector from the encoder to start decoding. We run the decoder as usual, producing symbols in our alphabet as well as the nonterminal symbol ``[n]'' and also the stop symbol. Whenever we see a nonterminal symbol, we add the current hidden state to a queue. Then, when we finish decoding (we see a stop symbol), we pop the next thing out of the queue and repeat. At the end of all this, we simply need to substitute back in decoded subtrees into the nonterminal symbols.

Whenever we recurse into a deeper level of decoding, the LSTMs is fed with the concatenation of two inputs: first, the usual input which is the last hidden state from the encoder, but also the hidden state of the parent LSTM when it generated that nonterminal symbol. 

\subsection{Neural Symbolic Machines \cite{NeuralSymbolicMachines}}

Neural symbolic machines are the high level counterpart to neural turing machines. Given a natural language query, they aim to generate a query in some language which when executed, will return the correct answer. This can be useful for answering questions from a large knowledge database, or when generalization ability is very important.

The main novelties of this paper are three-fold. First, is the manager-programmer-computer framework for solving this sort of problem. Second is the programmer architecture, which is a novel variant of your typical sequence2sequence model. Third is the training/optimization algorithm.

\todo{complete this}

\section{Grounding Project Papers Part 1}

\subsection{Red Wine and Red Tomatoes \cite{RedWine}}

This paper shows how to compose classifiers of visual primitives. A visual primitive can be a noun like ``car'' or an adjective like ``red''. The goal is to produce a classifier for ``red cars''. A naive composition using logical AND fails — this can be see by the fact that a small elephant is still larger than a large ant, so composing a size classifier with an animal classifier would fail unless the size classifier was provided with information about the typical size of each noun.

Instead, the authors decide to operate in model space. They define a transformation network which takes in two primitive classifiers and outputs a classifier for a complex concept. The transformation network operates on the weights of the two primitive classifiers. The reason we expect this approach to work is because the elephant classifier should have inside of it implicit information about how big an elephant is normally, and the transformation network will be able to use the information when constructing the small elephant classifier.

\subsection{Inductive Learning \cite{InductiveLearning}}

This paper speculates about the way that humans are able to understand concepts with very little data. According to the authors, humans have theories about how things work. Each theory corresponds to a distribution on hypotheses, and finally, we should use the hypothesis which fits to the data the best.

Each theory itself can be broken down into two levels — the structure level, and the principle level. The principle level might be something like: ``species should be organized into (taxonomic) trees''. The structure level would be the actual realization of that tree where we have figured out where each species goes on the tree. Then, a hypothesis under this theory might be: ``the word dog refers to all objects in this particular subtree of the taxonomic tree''. Usually, learning happens at the structure and hypothesis level. Theoretically, the principles of theories should be learnable too, but the authors don’t have any concrete suggestions on how to approach this. 

\subsection{Generating Visual Explanations \cite{VisualExplanations}}

This paper proposes a model for generating visual explanations. A visual explanation is a description of an image which explains a classification decision. For example: ``This is a Laysan Albatross because this bird has a hooked yellow beak wite neck and black back.''

The model consists of a feature extractor which produces a feature vector from the image. The feature vector is used to both classify the image and generate an explanation (via LSTM). Furthermore the explanation is then used to generate a description. Two losses are applied. The first loss is between the description and the target description, to encourage the generated explanation to be relevant to the image. The second loss involves classifying a sampled explanation sentence, and taking the loss between the image classification result and the explanation classification result. Since sampling was involved, training requires REINFORCE.

\subsection{Multimodal Embeddings \cite{MultimodalEmbeddings}}

This paper proposes a model for generating an embedding/representation from both a language description of something and an image of the same thing. For example, the word “car” along with pictures of a car. This is useful because then the derived embedding for a given concept is grounded in visual data, and there is evidence to suggest that humans reason in very visual ways. Concretely, the authors evaluate this by using the multimodal embeddings to compare similarity between concepts. 

The easiest way to create a multimodal embedding is to concatenate the word embedding with a feature vector derived from the images. However, this strategy only works on non-abstract concepts which can be visually represented, whereas the authors present a model which can also embed nonvisual concepts. The model consists of extracting a feature vector from some images, and a feature vector from the text (word2vec embeddings, for example). Then, a function $f$ is learned which maps language features to the image features to minimize MSE. The function is fixed over all concepts. Then, the multimodal embedding of a given concept computed as $f(l_w)$ where $l_w$ is the language embedding of that concept.

Although the authors used only a single word in the language embedding, I am curious to see if this idea would work with multiple words and an LSTM to generate the language embeddings.

\subsection{Abstract Concept Embeddings \cite{AbstractConceptEmbeddings}}

Like the last paper, this paper aims to embed concepts using both language and perceptual (visual) data. This task is made much more challenging when we try to embed abstract concepts like ``loyalty''. 

The authors use the skip-gram model, but augment the training data by taking annotated images, and then constructing artificial data using those annotations. For example, if an image of a screwdriver is annotated with parts of the screwdriver, the artificially constructed ``sentence'' might be something like: ``screwdriver handle screwdriver flat screwdriver long scredriver head.'' This encourages concepts which frequently co-occur in perceptual data to be close in embedding space.

Intuitively, this leads to good performance on abstract concepts because in the perceptual data, abstract concepts will appear frequently next to more concrete concepts which are associated with them. 

\subsection{The More You Know \cite{TheMoreYouKnow}}

This paper classifies images using both a CNN and a graph NN.  We are given a knowledge graph containing concepts and their relationships with each other. Each node in the graph is a concept, such as ``cat'' or ``blue''. The CNN outputs a number for every node in the knowledge graph to predict whether that concept is in the image. Then the GNN runs, and produces an output at each node. The outputs are concatenated and after a fully connected layer, the final classification is made.

The graph NN works as follows: Each node is initialized with the value predicted by the CNN. This is the initial state of that node. Then, a GRU/LSTM like procedure is used to update the state of each node based on the states of the neighbors. This happens until a certain number of time-steps have passed (or until convergence). Finally, the output is produced as a function of the final state and the initial input. The paper uses a novel variant of this model for computational efficiency reasons, but that is not important here.

\subsection{Learning to Reason \cite{LearningToReason}}

This paper approaches the problem of VQA by building upon a previous paper on neural module networks, which is composed of modules. A module is an NN which does a simple task — for example, classification of an object, counting the number of objects, comparing the sizes of two objects, etc. The idea is that any question can be answered by composing these modules in the correct way. For example, to answer “How many things are larger than the green ball”, we can compose the classification module, the size comparison module, and the counting module. 

Then, the remaining challenge is to compose or lay-out the modules in the best way to answer each question. The layout should be different for each question. This paper solves this problem by training a sequence to sequence model which takes as input the question and outputs the layout. This is trained using standard RL techniques.

\subsection{Tracking by Natural Language Specification \cite{TrackingSpecification}}

This paper is about tracking an object in a video when we are given text description of the object. There are three models introduced — the language only tracking model, the visual tracking model, and a multimodal tracking model. 

In the language only tracking model, we start with a text description, pass it through an LSTM, take the last hidden state, and then pass it through a feed-forward layer to produce a filter. This filter is then convolved with the input image, and the highest values in the output correspond to the location of the object. The filter is not updated over time.

In the visual tracking model, the language tracking model is used to locate the image patch containing the object at time 0, and then that image patch is used to create a new filter, which is used in the same way. This new filter is not updated over time.

In the multimodal tracking model, we update the filter at every time step by rerunning the LSTM at every time step, but with an attention mechanism. The input to this attention mechanism is the image patch with the object at the last frame, passed through a CNN. This allows the filter to change over time. However, since the text description is always present, semantic drift is avoided.

\section{Grounding Project Papers Part 2}

\subsection{Compositional Modular Networks \cite{CompositionalModularNetworks}}

This paper aims to solve the problem of referential expressions. For example, given an image, how can we locate the man with a blue shirt?

The paper breaks this up into a three part pipeline. In the first stage, the query is parsed into three feature vectors. The first vector $q_\text{subj}$contains information about the subject (the man). The second vector $q_\text{obj}$ contains information about the object (the blue shirt). Finally, the third vector $q_\text{rel}$ contains information about how the subject and the object are related. In this case, the relationship would be ``wearing''.

Simultaneously, a standard object detection network might propose many different bounding boxes in the scene. We loop through all ordered pairs of boxes $b_i$ and $b_j$ in the scene and seek to maximize the store $s(b_i, b_j)$, where $b_i$ is supposed to be the object and $b_j$ the subject. Once we find the maximum scoring pair, we take $b_i$ to be our final answer.

The score of two boxes is the sum of three terms -- how well the subject $b_i$ corresponds to the object vector $q_\text{subj}$, how well the object $b_j$ corresponds to $q_\text{obj}$, and how well both boxes are related to $q_\text{rel}$. In other words:

\eq{
  s(b_i, b_j) &= f_\text{loc}(b_i, q_\text{sub}) + f_\text{loc}(b_j, q_\text{obj}) + f_\text{rel}(b_i, b_j, q_\text{rel})
}

Where each of these $f$'s is parameterized by a network. All that remains is to describe the architecture of these networks and also the architecture of the question parsing network.

The parser (which produces each of the $q$'s) first passes the input word embeddings through a two-layer bidirectional LSTM. Therefore, if there are $n$ input words, we have $4n$ hidden states in our LSTM. We concatenate these hidden states together such that there are $n$ concatenated vectors.

Next, we have three branches -- for object, subject, and relation. Each branch is a fully connected layer followed by a softmax over the input words. This is the attention over the input words. The output of each branch is simply the weighted average of the input embeddings according to the softmax. 

The localization module $f_\text{loc}$ passes the image through a CNN, concatenates other relevant features (bounding box coordinates), passes it through another fully connected layer, element-wise multiplies it with the $q$ vector, and then passes it through two more fc layers. According to the authors, element-wise multiplication between language and visual vectors has been shown to be an effective way of creating multimodal representations.

The realsionship module does pretty much the same thing as the localization module, but since there are two input images, their vectors are concatenated together. Also, no visual features are used (only the bounding box spatial features are used). The authors claim that the visual features did not increase performance. 

\subsection{Visual7W \cite{Visual7W}}

This paper introduces a dataset, named in the title. The dataset consists of images, and each image is accompanied by a set of questions related to the image. In addition, each sentence may be attached to a relevant bounding box in the image. The answer format is multiple choice.

The basic model that the authors provide uses spatial attention to answer the questions. There are two stages to the algorithm -- encoding -- in which the image and question is simulataneously encoded -- and then the decoding stage, where the question is answered.

The encoding stage is simply an LSTM on the input question. However, it has an attention mechanism. At every time step, it produces an attention map on the image, which is used to integrate information from the image into the LSTM. The final hidden state of the LSTM is used for our encoding.

We also encode the answers -- I assume -- using a normal LSTM. Finally, we take dot products between the answers and the encoding vector and apply softmax.

\subsection{Unsupervised Reference Resolution in Videos \cite{ReferenceInVideos}}

This paper is super dense, and this summary does not come close to getting all the main ideas. However, in a nutshell, there are three parts to this model. First, an action graph which describes how actions and entities in the videos are related. We also have differential models $P(V|G)$ and $P(L|G)$ which give the likelihood of the video and the transcription given the action graph respectively. The task is reference resolution, which is equivalent to the task of finding the graph which maximizes $P(V|G)P(L|G)$. This optimization problem is performed using hard EM -- or alternating between optimizing the action graph with local search and optimizing the network parameters using standard backprop. 

\subsection{Relational Reasoning \cite{RelationalReasoning}}

\text{todo}

\section{MARL papers}

\subsection{NFSP \cite{NFSP}}

This paper brings the idea of ficticious self-play into deep reinforcement learning. It was not clear to me how exactly this worked -- more reading into the background of self play would be necessary to understand this, but the actual realization of this idea is quite simple -- the authors develop a variant of Q-learning.

This variant of Q-learning has three policies. First, the default Q-policy of picking the action with the highest Q. Second, the random strategy, which is typically used in Q learning for exploration purpoess. Finally, the average policy. The idea of self play is to choose the best response against the average policy, so we need to actually use the average policy.

During self play, we choose to perform $\eps$-greedy Q policy with chance $\eta$ and otherwise perform the average policy action. There are two replay memories -- one for all SARS tuples, used for training the Q netork, and one with only state action pairs chosen by the Q-policy, used for training the average policy to be closer to the Q-policy.

This algorithm achieves decent but not state of the art of some various forms of poker.

\subsection{BiCNet \cite{Bicnet}}

This paper is about playing the game StarCraft. To make a long story short, the main challenge of this game is coordinating multiple agents. Naively outputting a joint action is difficult because of the search space, so each agent more or less has its own network to output their action.

The main contribution of this paper is to add a bidirectional RNN halfway through the policy network which transfers information between each agent, so that they have knowledge of each others actions. The results indicate good performance. However, I am not sure that and RNN is a good way to manage this. It seems like this would scale poorly to say, 1000 agent scenarios. Instead, maybe some form of shared explicit memory between the agents would be a more efficient communication mechanism. 

\section{10-808 Papers}

\subsection{RNNs Can Learn Logical Semantics \cite{RNNSemantics}}

This paper is about a number of experiments on tree RNNs and tree RNTNs. Each of these tree networks takes as input two sentences, and the output is a classification problem of how the two sentences are related. For example, the first sentence might entail the second sentence, or it might contradict the second sentence, or the two sentences might be completely unrelated.

Each tree network consists of compositional layers, which are applied at every node in the parse tree of a sentence until the entire sentence is represented as a vector, and then the comparison layer is applied, and the final softmax classification comes immediately after.

The compositional layer can be described by the following equations. For RNN:
\eq{
  y_\text{RNN} = f(W_1 x_{\text{left}} + W_2 x_{\text{right}} + b)  
}
For the RNTN:
\eq{
  h_i = x_{\text{left}}^T W_i x_{\text{right}}
  y_\text{RNTN} = y_\text{RNN} + f(h)  
}

The authors run four sets of experiments. The first set of experiments deals with a simple set of propositions and their relation to each other. For example, if we know propositions $p_1$ and $p_2$ are equivalent to each other, and $p_1$ and $p_7$ are contradictory to each other, then if we get as inputs two sentences $p_2$ and $p_7$, the correct classification should be ``contradiction''. And, as the results show both NTN and NN do pretty well at this task, although the NTN generalizes significantly better.

In the second experiment, the authors train with compound propositions, so that we might have the sentence $p_4 \vee p_5$. Then, the composition layers come into play. Interestingly, the RNTN model does not generalize as well as the RNN model. In addition, both models -- when trained on sentences with at most 4 binary operations, and then tested on sentences ith up to 12 binary operations -- experience a significant amount of performance degredation. This is unfortunate, since we hoped that the right model would generalize to arbitrary length inputs.

In the third experiment, quantifiers were added to the language, and both RNNs and RNTNs did relatively well. Finally, in the fourth experiment, a real world dataset SICk was used, using actual natural language. The RNTN did well, and better than the RNN, but both did not do as well as the state of the art.

In my personal opinion, this paper showed that RNTNs do not seem to be able to ``compose'' semantic embeddings well, or better than RNNs. 

\bibliography{papers}
\end{document}
